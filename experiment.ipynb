{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import TicTacToeEnv\n",
    "from train import update_Q\n",
    "import random\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.model import Sequential\n",
    "\n",
    "env = gym.make('tictactoe-v0')\n",
    "\n",
    "def create_agent_model(learning_rate):\n",
    "    model = Sequential()\n",
    "\n",
    "    # input = one hot encoding of the board state (3*9 = 27 inputs)\n",
    "    # TODO: Use 2-d inputs so the encoding & relation between cells does not have to be learned by the model\n",
    "    model.add(layers.Dense(27, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    model.add(layers.Dense(27, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    model.add(layers.Dense(27, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    model.add(layers.Dense(27, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    model.add(layers.Dense(9, activation='linear', kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    \n",
    "    #sgd = optimizers.SGD(learning_rate=learning_rate, momentum=0.0, nesterov=False, name=\"SGD\")\n",
    "    adam =  optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = losses.MeanSquaredError()\n",
    "    model.compile(adam, loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.random_rate = 0\n",
    "        \n",
    "    def set_random_rate(self, rate):\n",
    "        self.random_rate = rate\n",
    "        \n",
    "    def get_action(self, env):\n",
    "        #print(\"Getting action for\")\n",
    "        #env.render()\n",
    "        if (self.random_rate > 0) and (random.random() < self.random_rate):\n",
    "#             print(\"random move\")\n",
    "            return random.randint(0,8)\n",
    "        else:\n",
    "            best_action = self._get_best_action()\n",
    "            return best_action\n",
    "    \n",
    "    def _get_best_action(self):\n",
    "        #print(\"predicted Q values\", self.model.predict(np.array([env._one_hot_board()])))\n",
    "        best_action = np.argmax(self.model.predict(np.array([env._one_hot_board()])), axis=1)\n",
    "        #print(\"best action\", best_action[0])\n",
    "        return best_action[0]\n",
    "        \n",
    "    def _get_random_action(self):\n",
    "        # truly random, so could yield invalid moves\n",
    "        return random.randint(0, 8)\n",
    "    \n",
    "    #TODO: get random *valid* action and get best *valid* action (to avoid invalid moves when not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_experience(env, agent1, agent2):\n",
    "    # plays a game until it's done, recording all steps into tuples (state, action, reward, next_state, done)\n",
    "    env.reset()\n",
    "    done = False\n",
    "    experience = []\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        \n",
    "        action = agent.get_action(env)\n",
    "        state = env._one_hot_board()\n",
    "        \n",
    "        step_result = env.step(action)\n",
    "        (next_state, reward, done, info) = step_result\n",
    "        \n",
    "        experience.append((state, action, reward, next_state, done))\n",
    "    return experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_agent_model(0.001)\n",
    "model2 = create_agent_model(0.001)\n",
    "agent1 = Agent(model1)\n",
    "agent2 = Agent(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: [2, 6, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [3, 1, 2, 1]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 6, 2]\n",
      "actions: [2, 1, 2]\n",
      "batch   0 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 0, 2]\n",
      "actions: [3, 0, 3]\n",
      "actions: [7, 0, 7]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 0, 2]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 1, 3]\n",
      "batch   1 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [8, 1, 3, 1]\n",
      "actions: [7, 0, 3, 0]\n",
      "actions: [3, 0, 3]\n",
      "actions: [7, 0, 7]\n",
      "actions: [3, 0, 3]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 3, 0]\n",
      "actions: [2, 0, 3, 7, 3]\n",
      "batch   2 /   35 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 5, 0]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [8, 0, 3, 0]\n",
      "actions: [3, 0, 3]\n",
      "actions: [2, 0, 3, 0]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "batch   3 /   33 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [5, 0, 3, 0]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "batch   4 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [4, 0, 3, 0]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 8, 0]\n",
      "actions: [7, 7]\n",
      "actions: [3, 7, 3]\n",
      "actions: [7, 7]\n",
      "batch   5 /   30 moves /   10 games /    0 a1 wins /    6 a1 fails /    0 a2 wins /    4 a2 fails /    0 draws \n",
      "actions: [4, 7, 4]\n",
      "actions: [2, 7, 2]\n",
      "actions: [7, 6, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 1, 7]\n",
      "actions: [7, 1, 7]\n",
      "actions: [7, 0, 5, 0]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "batch   6 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 2, 7]\n",
      "actions: [7, 0, 7]\n",
      "batch   7 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [7, 2, 7]\n",
      "actions: [7, 2, 7]\n",
      "actions: [7, 2, 7]\n",
      "actions: [3, 2, 7, 2]\n",
      "actions: [7, 2, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 2, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "batch   8 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "batch   9 /   29 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 2, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 1, 3]\n",
      "batch  10 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 2, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 0, 3]\n",
      "batch  11 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 2, 3]\n",
      "actions: [0, 0]\n",
      "actions: [0, 1, 0]\n",
      "actions: [0, 1, 0]\n",
      "actions: [3, 4, 3]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 1, 0]\n",
      "batch  12 /   29 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [0, 1, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 1, 0]\n",
      "actions: [0, 1, 0]\n",
      "actions: [0, 4, 0]\n",
      "actions: [3, 2, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 2, 3]\n",
      "batch  13 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [3, 1, 3]\n",
      "actions: [2, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [2, 1, 2]\n",
      "actions: [3, 4, 3]\n",
      "actions: [2, 7, 2]\n",
      "actions: [0, 1, 0]\n",
      "actions: [0, 1, 0]\n",
      "actions: [0, 1, 0]\n",
      "actions: [0, 1, 0]\n",
      "batch  14 /   29 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [0, 7, 0]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 6, 3]\n",
      "actions: [3, 4, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 2, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [1, 7, 3, 7]\n",
      "actions: [3, 7, 3]\n",
      "batch  15 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [2, 7, 2]\n",
      "actions: [0, 7, 0]\n",
      "actions: [5, 7, 3, 7]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [7, 7]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "batch  16 /   28 moves /   10 games /    0 a1 wins /    6 a1 fails /    0 a2 wins /    4 a2 fails /    0 draws \n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 7]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "batch  17 /   29 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "batch  18 /   29 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 7, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "batch  19 /   22 moves /   10 games /    0 a1 wins /    2 a1 fails /    0 a2 wins /    8 a2 fails /    0 draws \n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "batch  20 /   20 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "batch  21 /   20 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 0]\n",
      "actions: [0, 8, 0]\n",
      "actions: [3, 8, 3]\n",
      "actions: [2, 8, 2]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "batch  22 /   27 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "batch  23 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [3, 0, 1, 8, 7, 0]\n",
      "actions: [7, 0, 0]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "batch  24 /   33 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 2, 7]\n",
      "actions: [7, 0, 7]\n",
      "actions: [7, 2, 7]\n",
      "actions: [7, 2, 7]\n",
      "actions: [7, 0, 7]\n",
      "batch  25 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [7, 0, 7]\n",
      "actions: [4, 0, 4]\n",
      "actions: [0, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "batch  26 /   29 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 2, 0]\n",
      "actions: [0, 8, 0]\n",
      "actions: [4, 7, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: [6, 7, 6]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "batch  27 /   27 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "batch  28 /   20 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "batch  29 /   20 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 2, 8, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "batch  30 /   22 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 1, 2, 7]\n",
      "actions: [7, 7]\n",
      "batch  31 /   22 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [6, 3, 7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [7, 7]\n",
      "actions: [3, 7, 7]\n",
      "actions: [2, 7, 2]\n",
      "actions: [3, 7, 3]\n",
      "actions: [7, 7]\n",
      "batch  32 /   25 moves /   10 games /    0 a1 wins /    3 a1 fails /    0 a2 wins /    7 a2 fails /    0 draws \n",
      "actions: [6, 7, 6]\n",
      "actions: [7, 7]\n",
      "actions: [2, 7, 2]\n",
      "actions: [6, 7, 6]\n",
      "actions: [6, 7, 6]\n",
      "actions: [3, 2, 3]\n",
      "actions: [3, 1, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 2, 7]\n",
      "actions: [3, 7, 3]\n",
      "batch  33 /   30 moves /   10 games /    0 a1 wins /    8 a1 fails /    0 a2 wins /    2 a2 fails /    0 draws \n",
      "actions: [3, 5, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 5, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "batch  34 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "actions: [3, 5, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 2, 7]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "batch  35 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "actions: [7, 7]\n",
      "actions: [3, 7, 3]\n",
      "actions: [7, 7]\n",
      "actions: [3, 7, 3]\n",
      "actions: [2, 7, 3, 7]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 7, 3]\n",
      "batch  36 /   29 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 7, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n",
      "actions: [3, 0, 3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d055f4d9b44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mtrain_game_per_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d055f4d9b44d>\u001b[0m in \u001b[0;36mtrain_game_per_game\u001b[0;34m(env, agent1, agent2, num_batches, batch_size, verbose)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# train model after every game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d055f4d9b44d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(experiences, model, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"updated Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_game_per_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    800\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m    803\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \"\"\"\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3975\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3976\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3977\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   3978\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3219\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2529\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \"\"\"\n\u001b[0;32m-> 2531\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   2532\u001b[0m         *args, **kwargs)\n\u001b[1;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2496\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2497\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;31m# Variables to help check whether mutation happens in calling the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;31m# Copy the recursive list, tuple and map structure, but not base objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     func_args_before = nest.pack_sequence_as(func_args, flat_func_args,\n\u001b[0m\u001b[1;32m    921\u001b[0m                                              expand_composites=True)\n\u001b[1;32m    922\u001b[0m     func_kwargs_before = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   \"\"\"\n\u001b[0;32m--> 552\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     final_index, packed = _packed_nest_with_indices(structure, flat_sequence,\n\u001b[0m\u001b[1;32m    506\u001b[0m                                                     0, is_seq, sequence_fn)\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_packed_nest_with_indices\u001b[0;34m(structure, flat, index, is_seq, sequence_fn)\u001b[0m\n\u001b[1;32m    465\u001b[0m   \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m   \u001b[0msequence_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sequence_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_yield_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m       new_index, child = _packed_nest_with_indices(s, flat, index, is_seq,\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_yield_value\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_yield_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_yield_sorted_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_yield_sorted_items\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m \u001b[0mof\u001b[0m \u001b[0msorted\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \"\"\"\n\u001b[0;32m--> 211\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_collections_abc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;31m# Iterate through dictionaries in a deterministic order by sorting the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# keys. Notice this means that we ignore the original order of `OrderedDict`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/bin/../lib/python3.8/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  work in progress\n",
    "\n",
    "def split_experiences(experiences):\n",
    "    # split in even and odd (agent1 and agent2) experiences\n",
    "    return experiences[::2], experiences[1::2]\n",
    "    \n",
    "def train_model(experiences, model, verbose=False):\n",
    "    start_states = np.array([e[0] for e in experiences])\n",
    "    actions = np.array([e[1] for e in experiences])\n",
    "    rewards = np.array([e[2] for e in experiences])\n",
    "    next_states = np.array([e[3] for e in experiences])\n",
    "    dones = [e[4] for e in experiences]\n",
    "    Q = model.predict(start_states)\n",
    "    nextQ = model.predict(next_states)\n",
    "    \n",
    "    gamma = 0.95\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Q\", Q)\n",
    "        print(\"nextQ\", nextQ)\n",
    "        print(\"actions\", actions)\n",
    "        print(\"rewards\", rewards)\n",
    "        print(\"dones\", dones)\n",
    "    update_Q(Q, nextQ, actions, rewards, dones, gamma)\n",
    "    if verbose:\n",
    "        print(\"updated Q\", Q)\n",
    "    \n",
    "    model.fit(x=start_states, y=Q, epochs=10, verbose=verbose)\n",
    "\n",
    "def train_game_per_game(env, agent1, agent2, num_batches = 100, batch_size = 100, verbose=False):\n",
    "    moves = 0\n",
    "    gamelengths = []\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        agent1_fail = 0\n",
    "        agent2_fail = 0\n",
    "        draws = 0\n",
    "        \n",
    "        agent1_experience_batch  = []\n",
    "        agent2_experience_batch  = []\n",
    "        for game_num in range(batch_size):\n",
    "            experience = record_experience(env, agent1, agent2)\n",
    "            #print(len(experience))\n",
    "\n",
    "            #print actions when it was a draw (0 score)\n",
    "            #print(\"actions:\", [e[1] for e in experience])\n",
    "\n",
    "            if (experience[-1][2] == 0):\n",
    "                print(\"draw! actions:\", [e[1] for e in experience])\n",
    "            winner = env.get_winner()\n",
    "            if winner == 0:\n",
    "                print(\"agent 1 win! actions:\", [e[1] for e in experience])\n",
    "                agent1_wins += 1\n",
    "            elif winner == 1:\n",
    "                print(\"agent 2  win! actions:\", [e[1] for e in experience])\n",
    "                agent2_wins += 1\n",
    "            else:\n",
    "                if experience[-1][2] == -3:\n",
    "                    if env.current_player == 0:\n",
    "                        agent1_fail += 1\n",
    "                    else:\n",
    "                        agent2_fail += 1\n",
    "                else:\n",
    "                    draws+=1\n",
    "                    \n",
    "            moves += len(experience)\n",
    "\n",
    "            e1, e2 = split_experiences(experience)\n",
    "            agent1_experience_batch += e1\n",
    "            agent2_experience_batch += e2\n",
    "\n",
    "            # train model after every game\n",
    "            train_model(e1, agent1.model, verbose=verbose)\n",
    "            train_model(e2, agent2.model, verbose=verbose)\n",
    "\n",
    "    #       print(\"game actions\", [e[1] for e in experience])\n",
    "\n",
    "        \n",
    "        print(\"batch %3d / %4d moves / %4d games / %4d a1 wins / %4d a1 fails / %4d a2 wins / %4d a2 fails / %4d draws \"%( \n",
    "              batch_num, moves, batch_size, agent1_wins, agent1_fail, agent2_wins, agent2_fail, draws))\n",
    "        games = 0\n",
    "        moves = 0\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        draws = 0\n",
    "    \n",
    "agent1.set_random_rate(0.05)\n",
    "agent2.set_random_rate(0.05)\n",
    "train_game_per_game(env, agent1, agent2, 1000, 10, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent1, agent2, correct_invalid_actions = False):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        action = agent.get_action(env)\n",
    "        print(\"prediction\", agent.model.predict(np.array([env._one_hot_board()])))\n",
    "        print(\"action:\", action)\n",
    "        if env.board[action] is not None:\n",
    "            action = valid_actions[random.randint(0, len(valid_actions)-1)]\n",
    "            print(\"Agent picked invalid action. Adjusting with random value\")\n",
    "        obs,reward,done,info = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.set_random_rate(0)\n",
    "agent2.set_random_rate(0)\n",
    "play_game(env, agent1, agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
