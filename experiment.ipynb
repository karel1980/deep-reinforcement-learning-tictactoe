{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import TicTacToeEnv\n",
    "import random\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.model import Sequential\n",
    "\n",
    "env = gym.make('tictactoe-v0')\n",
    "\n",
    "def create_agent_model(learning_rate):\n",
    "    model = Sequential()\n",
    "\n",
    "    # input = one hot encoding of the board state (3*9 = 27 inputs)\n",
    "    # TODO: Use 2-d inputs so the encoding & relation between cells does not have to be learned by the model\n",
    "    model.add(layers.Dense(27, kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    model.add(layers.Dense(27, kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    model.add(layers.Dense(9, kernel_initializer=initializers.RandomNormal(stddev=0.01)))\n",
    "    \n",
    "    #sgd = optimizers.SGD(learning_rate=learning_rate, momentum=0.0, nesterov=False, name=\"SGD\")\n",
    "    adam =  optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = losses.MeanSquaredError()\n",
    "    model.compile(adam, loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.random_rate = 0\n",
    "        \n",
    "    def set_random_rate(self, rate):\n",
    "        self.random_rate = rate\n",
    "        \n",
    "    def get_action(self, env):\n",
    "        #print(\"Getting action for\")\n",
    "        #env.render()\n",
    "        if (self.random_rate > 0) and (random.random() < self.random_rate):\n",
    "#             print(\"random move\")\n",
    "            return random.randint(0,8)\n",
    "        else:\n",
    "            best_action = self._get_best_action()\n",
    "            return best_action\n",
    "    \n",
    "    def _get_best_action(self):\n",
    "        #print(\"predicted Q values\", self.model.predict(np.array([env._one_hot_board()])))\n",
    "        best_action = np.argmax(self.model.predict(np.array([env._one_hot_board()])), axis=1)\n",
    "        #print(\"best action\", best_action[0])\n",
    "        return best_action[0]\n",
    "        \n",
    "    def _get_random_action(self):\n",
    "        # truly random, so could yield invalid moves\n",
    "        return random.randint(0, 8)\n",
    "    \n",
    "    #TODO: get random *valid* action and get best *valid* action (to avoid invalid moves when not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_experience(env, agent1, agent2):\n",
    "    # plays a game until it's done, recording all steps into tuples (state, action, reward, next_state, done)\n",
    "    env.reset()\n",
    "    done = False\n",
    "    experience = []\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        \n",
    "        action = agent.get_action(env)\n",
    "        state = env._one_hot_board()\n",
    "        \n",
    "        step_result = env.step(action)\n",
    "        (next_state, reward, done, info) = step_result\n",
    "        \n",
    "        experience.append((state, action, reward, next_state, done))\n",
    "    return experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_agent_model(0.01)\n",
    "model2 = create_agent_model(0.01)\n",
    "agent1 = Agent(model1)\n",
    "agent2 = Agent(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0 /   49 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch   1 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "batch   2 /   34 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "batch   3 /   47 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch   4 /   37 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "batch   5 /   47 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "batch   6 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch   7 /   30 moves /   10 games /    0 a1 wins /    8 a1 fails /    0 a2 wins /    2 a2 fails /    0 draws \n",
      "batch   8 /   32 moves /   10 games /    0 a1 wins /    8 a1 fails /    0 a2 wins /    2 a2 fails /    0 draws \n",
      "batch   9 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "batch  10 /   44 moves /   10 games /    0 a1 wins /    2 a1 fails /    0 a2 wins /    8 a2 fails /    0 draws \n",
      "batch  11 /   39 moves /   10 games /    0 a1 wins /    1 a1 fails /    0 a2 wins /    9 a2 fails /    0 draws \n",
      "batch  12 /   56 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  13 /   47 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch  14 /   57 moves /   10 games /    0 a1 wins /    1 a1 fails /    0 a2 wins /    9 a2 fails /    0 draws \n",
      "batch  15 /   51 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch  16 /   60 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  17 /   36 moves /   10 games /    0 a1 wins /    6 a1 fails /    0 a2 wins /    4 a2 fails /    0 draws \n",
      "batch  18 /   37 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch  19 /   47 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch  20 /   40 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  21 /   50 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "batch  22 /   50 moves /   10 games /    1 a1 wins /    7 a1 fails /    0 a2 wins /    2 a2 fails /    0 draws \n",
      "batch  23 /   40 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  24 /   30 moves /   10 games /    0 a1 wins /   10 a1 fails /    0 a2 wins /    0 a2 fails /    0 draws \n",
      "batch  25 /   42 moves /   10 games /    1 a1 wins /    1 a1 fails /    0 a2 wins /    8 a2 fails /    0 draws \n",
      "batch  26 /   40 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  27 /   48 moves /   10 games /    0 a1 wins /    8 a1 fails /    0 a2 wins /    2 a2 fails /    0 draws \n",
      "batch  28 /   46 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  29 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch  30 /   59 moves /   10 games /    0 a1 wins /    3 a1 fails /    0 a2 wins /    7 a2 fails /    0 draws \n",
      "batch  31 /   20 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  32 /   40 moves /   10 games /    0 a1 wins /    0 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch  33 /   31 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch  34 /   47 moves /   10 games /    0 a1 wins /    5 a1 fails /    1 a2 wins /    4 a2 fails /    0 draws \n",
      "batch  35 /   49 moves /   10 games /    0 a1 wins /    9 a1 fails /    0 a2 wins /    1 a2 fails /    0 draws \n",
      "batch  36 /   42 moves /   10 games /    0 a1 wins /    6 a1 fails /    0 a2 wins /    4 a2 fails /    0 draws \n",
      "batch  37 /   48 moves /   10 games /    0 a1 wins /    2 a1 fails /    0 a2 wins /    8 a2 fails /    0 draws \n",
      "batch  38 /   44 moves /   10 games /    0 a1 wins /    2 a1 fails /    0 a2 wins /    8 a2 fails /    0 draws \n",
      "batch  39 /   49 moves /   10 games /    0 a1 wins /    7 a1 fails /    0 a2 wins /    3 a2 fails /    0 draws \n",
      "batch  40 /   45 moves /   10 games /    0 a1 wins /    5 a1 fails /    0 a2 wins /    5 a2 fails /    0 draws \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-097e8eab0336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mtrain_game_per_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-209-097e8eab0336>\u001b[0m in \u001b[0;36mtrain_game_per_game\u001b[0;34m(env, agent1, agent2, num_batches, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0magent2_experience_batch\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgame_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mexperience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;31m#print(len(experience))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-72fd432e4688>\u001b[0m in \u001b[0;36mrecord_experience\u001b[0;34m(env, agent1, agent2)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_hot_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-021f1593e33a>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-021f1593e33a>\u001b[0m in \u001b[0;36m_get_best_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print(\"predicted Q values\", self.model.predict(np.array([env._one_hot_board()])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_hot_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m#print(\"best action\", best_action[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1241\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    386\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mDataset\u001b[0m \u001b[0mof\u001b[0m \u001b[0minput\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0mmatching\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[0;32m--> 388\u001b[0;31m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mzip\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m    956\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \"\"\"\n\u001b[0;32m--> 958\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mZipDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m   3324\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.zip()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3325\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3326\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3328\u001b[0m           message = (\"The argument to `Dataset.zip()` must be a nested \"\n",
      "\u001b[0;32m~/virtualenvs/default/bin/../lib/python3.8/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  work in progress\n",
    "\n",
    "def split_experiences(experiences):\n",
    "    # split in even and odd (agent1 and agent2) experiences\n",
    "    return experiences[::2], experiences[1::2]\n",
    "    \n",
    "def train_model(experiences, model, verbose=False):\n",
    "    start_states = np.array([e[0] for e in experiences])\n",
    "    actions = np.array([e[1] for e in experiences])\n",
    "    rewards = np.array([e[2] for e in experiences])\n",
    "    next_states = np.array([e[3] for e in experiences])\n",
    "    dones = [e[4] for e in experiences]\n",
    "    # Only use the next Q value if the game is not done\n",
    "    use_next = np.array([0 if e[4] else 1 for e in experiences])\n",
    "    Q = model.predict(start_states)\n",
    "    nextQ = model.predict(next_states)\n",
    "    #print(\"nextQ\", nextQ)\n",
    "    \n",
    "    gamma = 0.95\n",
    "    \n",
    "    #print(\"original Q\", Q)\n",
    "    #print(\"actions\", actions)\n",
    "    for row in range(len(Q)):\n",
    "        # Update Q values for observed rewards + nextQ if game was not done\n",
    "        # FIXME: Q[:,actions] = ... does  not what it should do\n",
    "        Q[row,actions[row]] = rewards[row]  + 0 if dones[row] else (gamma * np.max(nextQ[row]))\n",
    "    #print(\"updated Q\", Q)\n",
    "    \n",
    "    model.fit(x=start_states, y=Q, epochs=10, verbose=verbose)\n",
    "\n",
    "    #newQ = model.predict(start_states)\n",
    "    #print(\"Q after fit\", newQ)\n",
    "    #print(\"best action after fit\", np.argmax(newQ, axis = 1))\n",
    "    \n",
    "def train_game_per_game(env, agent1, agent2, num_batches = 100, batch_size = 100):\n",
    "    moves = 0\n",
    "    gamelengths = []\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        agent1_fail = 0\n",
    "        agent2_fail = 0\n",
    "        draws = 0\n",
    "        \n",
    "        agent1_experience_batch  = []\n",
    "        agent2_experience_batch  = []\n",
    "        for game_num in range(batch_size):\n",
    "            experience = record_experience(env, agent1, agent2)\n",
    "            #print(len(experience))\n",
    "\n",
    "            #print actions when it was a draw (0 score)\n",
    "            if (experience[-1][2] == 0):\n",
    "                print(\"actions in game\", [e[1] for e in experience])\n",
    "            winner = env.get_winner()\n",
    "            if winner == 0:\n",
    "                agent1_wins += 1\n",
    "            elif winner == 1:\n",
    "                agent2_wins += 1\n",
    "            else:\n",
    "                if experience[-1][2] == -3:\n",
    "                    if env.current_player == 0:\n",
    "                        agent1_fail += 1\n",
    "                    else:\n",
    "                        agent2_fail += 1\n",
    "                else:\n",
    "                    draws+=1\n",
    "                    \n",
    "            moves += len(experience)\n",
    "\n",
    "            e1, e2 = split_experiences(experience)\n",
    "            agent1_experience_batch += e1\n",
    "            agent2_experience_batch += e2\n",
    "\n",
    "    #       print(\"game actions\", [e[1] for e in experience])\n",
    "\n",
    "        # train model after every game\n",
    "        train_model(agent1_experience_batch, agent1.model)\n",
    "        train_model(agent2_experience_batch, agent2.model)\n",
    "        \n",
    "        print(\"batch %3d / %4d moves / %4d games / %4d a1 wins / %4d a1 fails / %4d a2 wins / %4d a2 fails / %4d draws \"%( \n",
    "              batch_num, moves, batch_size, agent1_wins, agent1_fail, agent2_wins, agent2_fail, draws))\n",
    "        games = 0\n",
    "        moves = 0\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        draws = 0\n",
    "    \n",
    "agent1.set_random_rate(0.1)\n",
    "agent2.set_random_rate(0.1)\n",
    "train_game_per_game(env, agent1, agent2, 100, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent1, agent2, correct_invalid_actions = False):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        action = agent.get_action(env)\n",
    "        print(\"prediction\", agent.model.predict(np.array([env._one_hot_board()])))\n",
    "        print(\"action:\", action)\n",
    "        if env.board[action] is not None:\n",
    "            action = valid_actions[random.randint(0, len(valid_actions)-1)]\n",
    "            print(\"Agent picked invalid action. Adjusting with random value\")\n",
    "        obs,reward,done,info = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.set_random_rate(0)\n",
    "agent2.set_random_rate(0)\n",
    "play_game(env, agent1, agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextQ = np.array([[1,2,3], [4,5,6]])\n",
    "use_next = np.array([1,0])\n",
    "use_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[1,2,3], [4,5,6]])\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4],\n",
       "       [5, 6, 7]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextQ  = np.array([[2,3,4],[5,6,7]])\n",
    "nextQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextQ *  use_next[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = np.array([1, 2])\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([[2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-90604ca86487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "Q[actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 2.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)  *  actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'rand'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-4b9db0417ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    215\u001b[0m                                      \"{!r}\".format(__name__, attr))\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'rand'"
     ]
    }
   ],
   "source": [
    "np.rand([4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52275479, 0.64401731, 0.27550146, 0.15703939],\n",
       "       [0.25376878, 0.51209294, 0.63056784, 0.99384943],\n",
       "       [0.40274882, 0.16589087, 0.65885759, 0.09049816]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q  =  np.random.rand(3,4)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [2,3,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59155955, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4837838 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.63966218, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.44093435]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(4) * Q[:, actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cross index must be 1 dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-bdacb6b29223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mix_\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/default/lib/python3.8/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36mix_\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross index must be 1 dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cross index must be 1 dimensional"
     ]
    }
   ],
   "source": [
    "np.ix_(np.array(actions), Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0],\n",
       "        [1]]),\n",
       " array([[1, 2]]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ix_([0,1],[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[0]]],\n",
       " \n",
       " \n",
       "        [[[2]]]]),\n",
       " array([[[[1]],\n",
       " \n",
       "         [[3]]]]),\n",
       " array([[[[2],\n",
       "          [0]]]]),\n",
       " array([[[[3, 1]]]]))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = list(enumerate(actions))\n",
    "k\n",
    "a = np.ix_(*k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-b952fd8a82c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "Q[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
