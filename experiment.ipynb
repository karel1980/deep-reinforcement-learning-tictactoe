{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import TicTacToeEnv\n",
    "import random\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.model import Sequential\n",
    "\n",
    "env = gym.make('tictactoe-v0')\n",
    "\n",
    "def create_agent_model(learning_rate):\n",
    "    model = Sequential()\n",
    "\n",
    "    # input = one hot encoding of the board state (3*9 = 27 inputs)\n",
    "    # TODO: Use 2-d inputs so the encoding & relation between cells does not have to be learned by the model\n",
    "    model.add(layers.Dense(27, activation=\"relu\"))\n",
    "    model.add(layers.Dense(100, activation=\"relu\"))\n",
    "    model.add(layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(layers.Dense(20, activation=\"relu\"))\n",
    "    model.add(layers.Dense(9))\n",
    "    \n",
    "    optimizer =  optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = losses.MeanSquaredError()\n",
    "    model.compile(optimizer, loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.random_rate = 0.01\n",
    "        \n",
    "    def set_random_rate(self, rate):\n",
    "        self.random_rate = rate\n",
    "        \n",
    "    def get_action(self, env):\n",
    "        if self.random_rate > 0 and random.random() < self.random_rate:\n",
    "            return random.randint(0,8)\n",
    "        else:\n",
    "            best_action = self._get_best_action()\n",
    "            return best_action\n",
    "    \n",
    "    def _get_best_action(self):\n",
    "        best_action = np.argmax(self.model.predict(np.array([env._one_hot_board()])), axis=1)\n",
    "        return best_action[0]\n",
    "        \n",
    "    def _get_random_action(self):\n",
    "        # truly random, so could yield invalid moves\n",
    "        return random.randint(0, 8)\n",
    "    \n",
    "    #TODO: get random *valid* action and get best *valid* action (to avoid invalid moves when not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_experience(env, agent1, agent2):\n",
    "    # plays a game until it's done, recording all steps into tuples (state, action, reward, next_state, done)\n",
    "    env.reset()\n",
    "    done = False\n",
    "    experience = []\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        \n",
    "        action = agent.get_action(env)\n",
    "        state = env._one_hot_board()\n",
    "        \n",
    "        step_result = env.step(action)\n",
    "        (next_state, reward, done, info) = step_result\n",
    "        \n",
    "        #print(\"TTTT\", action)\n",
    "        experience.append((state, action, reward, next_state, done))\n",
    "    return experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_agent_model(0.01)\n",
    "model2 = create_agent_model(0.01)\n",
    "agent1 = Agent(model1)\n",
    "agent2 = Agent(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch   0 /  349 moves /  100 games /    0 a1 wins /   67 a1 fails /    0 a2 wins /   33 a2 fails /    0 draws \n",
      "batch   1 /  306 moves /  100 games /    0 a1 wins /   90 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch   2 /  316 moves /  100 games /    0 a1 wins /   90 a1 fails /    0 a2 wins /   10 a2 fails /    0 draws \n",
      "batch   3 /  311 moves /  100 games /    0 a1 wins /   89 a1 fails /    0 a2 wins /   11 a2 fails /    0 draws \n",
      "batch   4 /  310 moves /  100 games /    0 a1 wins /   94 a1 fails /    0 a2 wins /    6 a2 fails /    0 draws \n",
      "batch   5 /  306 moves /  100 games /    0 a1 wins /   94 a1 fails /    0 a2 wins /    6 a2 fails /    0 draws \n"
     ]
    }
   ],
   "source": [
    "#  work in progress\n",
    "\n",
    "def split_experiences(experiences):\n",
    "    # split in even and odd (agent1 and agent2) experiences\n",
    "    return experiences[::2], experiences[1::2]\n",
    "    \n",
    "def train_model(experiences, model, verbose=False):\n",
    "    start_states = np.array([e[0] for e in experiences])\n",
    "    actions = np.array([e[1] for e in experiences])\n",
    "    rewards = np.array([e[2] for e in experiences])\n",
    "    next_states = np.array([e[3] for e in experiences])\n",
    "    dones = [e[4] for e in experiences]\n",
    "    Q = model.predict(start_states)\n",
    "    nextQ = model.predict(next_states)\n",
    "    \n",
    "    gamma = 0.95\n",
    "#     print(\"current Q\", Q)\n",
    "#     print(\"current best action\", np.argmax(Q, axis=1))\n",
    "#     print(\"experiences\", experiences)\n",
    "#     print(\"next Q\", nextQ)\n",
    "#     print(\"best next Q\", np.max(nextQ, axis = 1))\n",
    "\n",
    "    # Update Q values for observed rewards of actions\n",
    "    # If the game is done we don't need to add the next Q value\n",
    "    \n",
    "    # TODO: this could also be done array manipulations; for loop bad. numpy good.\n",
    "    for (s0, a, r, s1, Qs0, Qs1, done) in zip(start_states, actions, rewards, next_states, Q, nextQ, dones):\n",
    "#         print(\"old Q line\", Qs0)\n",
    "        Qs0[a] = r + (0 if done else gamma * np.max(Qs1))\n",
    "#         print(\"new Q line\", Qs0)\n",
    "        # fit one by one. This  is a lot slower than fitting in batch\n",
    "        #model.fit(x=np.array([s0]), y=np.array([Qs0]), epochs=1, verbose=verbose)\n",
    "        \n",
    "#     print(\"updated Q\", Q),\n",
    "#     print(\"updated Q\", np.argmax(Q, axis=1))\n",
    "    model.fit(x=start_states, y=Q, epochs=1, verbose=verbose)\n",
    "        \n",
    "#     newQ = model.predict(start_states)\n",
    "#     print(\"Q after fit\", newQ)\n",
    "#     print(\"best action after fit\", np.argmax(newQ, axis = 1))\n",
    "    \n",
    "def train_game_per_game(env, agent1, agent2, num_batches = 100, batch_size = 100):\n",
    "    moves = 0\n",
    "    gamelengths = []\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        agent1_fail = 0\n",
    "        agent2_fail = 0\n",
    "        draws = 0\n",
    "        \n",
    "        for game_num in range(batch_size):\n",
    "            experience = record_experience(env, agent1, agent2)\n",
    "            winner = env.get_winner()\n",
    "            if winner == 0:\n",
    "                agent1_wins += 1\n",
    "            elif winner == 1:\n",
    "                agent2_wins += 1\n",
    "            else:\n",
    "                if experience[-1][2] == -3:\n",
    "                    if env.current_player == 0:\n",
    "                        agent1_fail += 1\n",
    "                    else:\n",
    "                        agent2_fail += 1\n",
    "                else:\n",
    "                    draws+=1\n",
    "                    \n",
    "            moves += len(experience)\n",
    "\n",
    "            e1, e2 = split_experiences(experience)\n",
    "\n",
    "    #       print(\"game actions\", [e[1] for e in experience])\n",
    "\n",
    "            # train model after every game\n",
    "            train_model(e1, agent1.model)\n",
    "            train_model(e2, agent2.model)\n",
    "\n",
    "        \n",
    "        print(\"batch %3d / %4d moves / %4d games / %4d a1 wins / %4d a1 fails / %4d a2 wins / %4d a2 fails / %4d draws \"%( \n",
    "              batch_num, moves, batch_size, agent1_wins, agent1_fail, agent2_wins, agent2_fail, draws))\n",
    "        games = 0\n",
    "        moves = 0\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        draws = 0\n",
    "                    \n",
    "agent1.set_random_rate(0.02)\n",
    "agent2.set_random_rate(0.02)\n",
    "train_game_per_game(env, agent1, agent2, 100, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent1, agent2):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        action = agent.get_action(env)\n",
    "        print(\"prediction\", agent.model.predict(np.array([env._one_hot_board()])))\n",
    "        print(\"action:\", action)\n",
    "        obs,reward,done,info = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.set_random_rate(0)\n",
    "agent2.set_random_rate(0)\n",
    "play_game(env, agent1, agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
