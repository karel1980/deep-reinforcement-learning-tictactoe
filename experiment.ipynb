{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import TicTacToeEnv\n",
    "import random\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.model import Sequential\n",
    "\n",
    "env = gym.make('tictactoe-v0')\n",
    "\n",
    "def create_agent_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # input = one hot encoding of the board state (3*9 = 27 inputs)\n",
    "    model.add(layers.Dense(27, activation=\"relu\"))\n",
    "    model.add(layers.Dense(100, activation=\"relu\"))\n",
    "    model.add(layers.Dense(100, activation=\"relu\"))\n",
    "    model.add(layers.Dense(9))\n",
    "    \n",
    "    optimizer =  optimizers.Adam(learning_rate=0.01)\n",
    "    loss = losses.MeanSquaredError()\n",
    "    model.compile(optimizer, loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.random_rate = 0.01\n",
    "        \n",
    "    def set_random_rate(self, rate):\n",
    "        self.random_rate = rate\n",
    "        \n",
    "    def get_action(self, env):\n",
    "        if self.random_rate > 0 and random.random() < self.random_rate:\n",
    "            return random.randint(0,8)\n",
    "        else:\n",
    "            best_action = self._get_best_action()\n",
    "            return best_action\n",
    "    \n",
    "    def _get_best_action(self):\n",
    "        best_action = np.argmax(self.model.predict(np.array([env._one_hot_board()])), axis=1)\n",
    "        return best_action[0]\n",
    "        \n",
    "    def _get_random_action(self):\n",
    "        # truly random, so could yield invalid moves\n",
    "        return random.randint(0, 8)\n",
    "    \n",
    "    #TODO: get random *valid* action and get best *valid* action (to avoid invalid moves when not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_experience(env, agent1, agent2):\n",
    "    # plays a game until it's done, recording all steps into tuples (state, action, reward, next_state, done)\n",
    "    env.reset()\n",
    "    done = False\n",
    "    experience = []\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        action = agent.get_action(env)\n",
    "        state = env._one_hot_board()\n",
    "        step_result = env.step(action)\n",
    "        (next_state, reward, done, info) = step_result\n",
    "        \n",
    "        #print(\"TTTT\", action)\n",
    "        experience.append((state, action, reward, next_state, done))\n",
    "    return experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_agent_model()\n",
    "model2 = create_agent_model()\n",
    "agent1 = Agent(model1)\n",
    "agent2 = Agent(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: 343 moves in 100 games / agent1 won 0 / agent2 won 100\n",
      "batch 1: 403 moves in 100 games / agent1 won 1 / agent2 won 99\n",
      "batch 2: 407 moves in 100 games / agent1 won 0 / agent2 won 100\n",
      "batch 3: 370 moves in 100 games / agent1 won 1 / agent2 won 99\n",
      "batch 4: 360 moves in 100 games / agent1 won 0 / agent2 won 100\n",
      "batch 5: 357 moves in 100 games / agent1 won 0 / agent2 won 100\n",
      "batch 6: 375 moves in 100 games / agent1 won 1 / agent2 won 99\n",
      "batch 7: 374 moves in 100 games / agent1 won 1 / agent2 won 99\n",
      "batch 8: 373 moves in 100 games / agent1 won 1 / agent2 won 99\n",
      "batch 9: 332 moves in 100 games / agent1 won 0 / agent2 won 100\n",
      "batch 10: 369 moves in 100 games / agent1 won 0 / agent2 won 100\n",
      "batch 11: 348 moves in 100 games / agent1 won 1 / agent2 won 99\n"
     ]
    }
   ],
   "source": [
    "#  work in progress\n",
    "\n",
    "def split_experiences(experiences):\n",
    "    # split in even and odd (agent1 and agent2) experiences\n",
    "    return experiences[::2], experiences[1::2]\n",
    "    \n",
    "def train_model(experiences, model, verbose=False):\n",
    "    start_states = np.array([e[0] for e in experiences])\n",
    "    actions = np.array([e[1] for e in experiences])\n",
    "    rewards = np.array([e[2] for e in experiences])\n",
    "    next_states = np.array([e[3] for e in experiences])\n",
    "    dones = [e[4] for e in experiences]\n",
    "    Q = model.predict(start_states)\n",
    "    nextQ = model.predict(next_states)\n",
    "    \n",
    "    gamma = 0.95\n",
    "    #print(\"experiences\", experiences)\n",
    "    #print(\"current Q\", Q)\n",
    "    #print(\"current best action\", np.argmax(Q, axis=1))\n",
    "    #print(\"next Q\", nextQ)\n",
    "    #print(\"best next Q\", np.max(nextQ, axis = 1))\n",
    "\n",
    "    # Update Q values for observed rewards of actions\n",
    "    # If the game is done we don't need to add the next Q value\n",
    "    for (s0, a, r, s1, Qs0, Qs1, done) in zip(start_states, actions, rewards, next_states, Q, nextQ, dones):\n",
    "        Qs0[a] = r + 0 if done else gamma * np.max(Qs1)\n",
    "        model.fit(x=np.array([s0]), y=np.array([Qs0]), epochs=1, verbose=verbose)\n",
    "\n",
    "    #print(\"updated Q\", Q),\n",
    "    #print(\"updated Q\", np.argmax(Q, axis=1))\n",
    "    newQ = model.predict(start_states)\n",
    "    #print(\"Q after fit\", newQ)\n",
    "    #print(\"best action after fit\", np.argmax(newQ, axis = 1))\n",
    "    \n",
    "def train_game_per_game(env, agent1, agent2):\n",
    "    moves = 0\n",
    "    gamelengths = []\n",
    "    \n",
    "    #fig, ax = plt.subplots()\n",
    "    #fig.show()\n",
    "    \n",
    "    games =  0\n",
    "   \n",
    "    NUM_BATCHES = 10000\n",
    "    BATCH_SIZE = 100\n",
    "    for batch_num in range(NUM_BATCHES):\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        for game in range(BATCH_SIZE):\n",
    "            experience = record_experience(env, agent1, agent2)\n",
    "            #print(\"AAAA\", experience)\n",
    "            winner = env.get_winner()\n",
    "            if winner == 0:\n",
    "                agent1_wins += 1\n",
    "            else:\n",
    "                agent2_wins += 1\n",
    "            games += 1\n",
    "            moves += len(experience)\n",
    "\n",
    "            e1, e2 = split_experiences(experience)\n",
    "            #print(\"BBBB\", e1)\n",
    "            #print(\"CCCC\", e2)\n",
    "            \n",
    "            train_model(e1, agent1.model)\n",
    "            train_model(e2, agent2.model)\n",
    "\n",
    "        print(\"batch %d: %d moves in %d games / agent1 won %d / agent2 won %d\"%(batch_num, moves, games, agent1_wins, agent2_wins))\n",
    "        games = 0\n",
    "        moves = 0\n",
    "        agent1_wins = 0\n",
    "        agent2_wins = 0\n",
    "        \n",
    "        #Add a few plots (game length for each iteration, win/lose stats for each agent)\n",
    "            \n",
    "agent1.set_random_rate(0.1)\n",
    "agent2.set_random_rate(0.1)\n",
    "#train_game_per_game(10000, env, agent1, agent2)\n",
    "train_game_per_game(env, agent1, agent2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent1, agent2):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        agent = agent1 if env.current_player == 0 else agent2\n",
    "        action = agent.get_action(env)\n",
    "        print(\"prediction\", agent.model.predict(np.array([env._one_hot_board()])))\n",
    "        print(\"action:\", action)\n",
    "        obs,reward,done,info = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.set_random_rate(0)\n",
    "agent2.set_random_rate(0)\n",
    "play_game(env, agent1, agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
